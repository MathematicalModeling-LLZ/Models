博客地址：https://blog.csdn.net/xxiangyusb/article/details/99762451
# 应用场景
简单地说，回归分析是对拟合问题做的一种统计分析。
P.S. 曲线拟合问题的特点是，根据得到的若干有关变量的一组数据，寻找因变量与（一个或几个）自变量之间一个函数，使这个函数对那组数据拟合得最好。通常。**函数的形式可以由经验、先验知识或对数据的直接观察**决定，要做的工作是由数据用最小二乘法**计算函数中的待定系数**。

具体地说，回归分析在一组数据的基础上研究以下问题：
1. 建立因变量$y$与自变量$x_1,x_2,...,x_m$之间的回归模型（经验公式）；
2. 对回归模型的可信度进行检验；
3. 判断每个自变量$x_i(i=1,2,...,m)$对$y$的影响是否显著；
4. 诊断回归模型是否适合这组数据；
5. 利用回归模型对$y$进行预报或控制。

# 1. 建立回归模型
## 1.1 筛选变量
### 1.1.1 确定样本空间
$m$个变量，对它们分别进行了$n$次采样（或观测），得到$n$个样本点，
$$(x_{i1}, x_{i2}, ... , x_{im}),   i = 1, 2, ..., n$$
所构成的数据表可以写成一个$n \times m$维的矩阵。
### 1.1.2 对数据进行标准化处理
（1）**数据的中心化处理**
实际上就是平移变化，即$x_{ij}^* = x_{ij} - \overline{x_j},  i=1,2,...,n,  j=1,2,...,m$

这种处理，可以是样本的均值为$0$，同时它既不改变样本点的相互位置，也不改变变量间的相关性，但变换后，有许多技术上的便利。
（2）**数据的无量纲化处理**
在实际问题中，不同变量的测量单位往往是不同的。
为了消除变量的量纲效应，使每个变量都具有同等的表现力，数据分析中常用的消量纲的方法，是对不同的变量进行所谓的**压缩处理——使每个变量的方差为1**
即，$$x_{ij}^* = x_{ij} / s_j，其中，s_j = \sqrt{\frac{1}{n-1}\sum_{i=1}^{n}(x_{ij}-\overline{x_j})^2}$$

当然，也有其他消量纲的方法，此处不一一列举。
（3）**数据的标准化处理**——对数据同时进行“中心化-压缩”处理
即，$$x_{ij}^* - \frac{x_{ij} - \overline{x_j}}{s_j},  i=1,2,...,n,  j=1,2,...m$$
### 1.1.3 变量筛选
——选择哪些变量作为因变量的解释变量：
 - 一方面，希望尽可能不遗漏重要的解释变量
 - 一方面，遵循参数节省原则(自变量数目过大时，模型计算复杂，且往往会扩大估计方差，降低模型精度)，使自变量的个数尽可能少

（1）穷举法
列举出所有可能的潜在变量，再根据自变量的不同组合，选取合适的模型。
假设有$m$个潜在变量，则需要拟合与比较的方程个数为$2_m$——当$m$较大时不现实

（2）向前选择变量法
```mermaid
flowchat
st=>start: 初始：模型中没有任何解释变量
e=>end: 结束
op1=>operation: 分别考虑y与每一个自变量的一元线性回归模型
op2=>operation: 对所有的这m个模型进行F检验，选择F值最高者作为第一个进入模型的自变量
op3=>operation: 对剩下的变量分别进行偏F检验
op4=>operation: 在所有通过偏F检验的自变量中，选择Fj值最大者作为下一个被选入模型的自变量
cond=>condition: 至少有一个xi通过了偏F检验？

st->op1->op2->op3->cond
cond(no)->e
cond(yes)->op4->op3
```
缺点：
一旦某个自变量被选入模型，它就永远留在模型中。然鹅，随着其他变量的引入，由于变量之间相互传递的相关关系，一些先进入模型的变量的解释作用可能会变得不再显著。

（3）向后删除变量法
```mermaid
flowchat
st=>start: 初始：所有自变量都在模型中（起始的全模型）
e=>end: 结束
op1=>operation: 分别对模型中剩余的每一个自变量做偏F检验（以去掉xj的模型为减模型）
op2=>operation: 选择Fj值最小的自变量，将它从模型中删除
cond=>condition: 所有的变量都通过了偏F检验？

st->op1->cond
cond(no)->e
cond(yes)->op2->op1
```
缺点：
一旦某个自变量被删除后，它就永远被排斥在模型之外。但是，随着其它变量的被删除，它对 y 的解释作用也可能会显著起来。

（4）逐步回归法——最常用

综合向前选择和向后删除，采取边进边退的方法：
 - 对于模型外部的变量，只要它还可以提供显著的解释信息，就可以再次进入模型
 - 对于已在内部的变量，只要它的偏F检验不能通过，则还可能从模型中删除

具体流程见书，此处不再赘述。

另外，为了避免变量的进出循环，一般取偏F检验拒绝域的临界值为：$F_进 > F_出$，式中，$F_进$为选入变量时的临界值，$F_出$未删除变量时的临界值。


在所有标准的统计软件中都有逐步回归的程序。$F_进$和$F_出$的检验水平值也可以自定，也可以是备择的。常见的检验水平值为$\alpha_进 = 0.05$，$\alpha_出 = 0.1$
### 1.1.4 调整复判定系数
——一般的统计软件常在输出中同时给出$R^2$和$\overline{R}^2$，如果两者相差过大，则应考虑减少或调整变量【个人认为，可用于检验逐步回归的结果】

统计学家主张在回归建模时，**采用尽可能少的自变量，不要盲目地追求复判定系数$R^2$的提高**。
当变量增加时，残差项的自由度就会减少$df_E = n-m-1$，自由度越小，数据的统计趋势就越不容易显现，故而定义了一个**调整复判定系数**：

$$\overline{R}^2 = 1 - \frac{Q/(n-m-1)}{SST/(n-1)}$$

此外，$\overline{R}^2$还可以用于判断是否可以再增加新的变量：
若增加一个变量，
 - $\overline{R}^2$明显增加，，可考虑增加此变量
 - $\overline{R}^2$无明显变化，不必增加此变量

## 1.2 最小二乘估计
一元线性回归、多元线性回归——略。
# 2. 回归模型假设检验
 ——检查自变量与因变量之间能否用一个线性关系模型表示（$F$检验）
 
 具体检验方法见书，此处不再赘述。
# 3. 回归参数假设检验和区间估计
——检查每一个自变量对因变量的影响是否显著（$t$ 检验）

具体检验方法见书，此处不再赘述。
# 4. 拟合效果分析
## 4.1 残差的样本方差(MSE)
$$MSE = \frac{1}{n-2} \sum_{i=1}^{n}(e_i - \overline{e})^2$$
可以计算残差的样本均值 $\overline{e} = 0$
记，$$S_e = \sqrt{MSE} = \sqrt{\frac{1}{n-2} \sum_{i=1}{n} {e_i}^2}$$
$S_e$越小，拟合效果越好
## 4.2 判定系数（拟合优度）
——指可解释的变异占总变异的百分比，用$R^2$表示
$$R^2 = \frac{SSR}{SST} = 1 - \frac{SSE}{SST}$$
其中，
$$SST = \sum_{i=1}^n(y_i - \overline{y})^2，原始数据y_i的总变异平方和，df_T = n-1$$
$$SSR = \sum_{i=1}^n(\hat{y_i}-\overline{y})^2，用拟合直线可解释的变异平方和，df_R = 1$$
$$SSE = \sum_{i=1}^n(y_i - \hat{y_i})^2，残差平方和，df_E = n-2$$
$$SST = SSR + SSE$$  

$R^2$越接近1，拟合点与原数据越吻合

另外，还可证明，$\sqrt{R^2}$等于$y$与自变量$x$的相关系数，而相关系数的正负号与回归系数$\hat{\beta_1}$的符号相同
# 5. 利用回归模型进行预测
![利用回归模型预测](https://img-blog.csdnimg.cn/20190826162620804.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3h4aWFuZ3l1c2I=,size_16,color_FFFFFF,t_70#pic_center)
# 其他
## 偏相关系数（净相关系数）
在研究两个变量之间的线性相关程度时，可考察这两个变量的简单相关系数。但在研究多个变量之间的线性相关程度时，单纯使用两两变量的简单相关系数往往具有虚假性。因为它只考虑了两个变量之间的相互作用，忽略了其他变量对这两个变量的影响。
## 复共线性和有偏估计方法
在一些大型线性回归问题中，最小二乘估计不总令人满意，比如系数正负号与实际意义不符，这可能是因为**回归自变量之间存在着近似线性关系**——**复共线性(Multicollinearity)**

解决方法——牺牲无偏性，改用合适的有偏估计方法，以改善估计的稳定性
例如，**岭估计**——可以显著改善矩阵列复共线性时最小二乘估计量的均方误差，增强估计的稳定性。
（P.S. 均方误差Mean Squared Errors：一个好的估计应该具有较小的均方误差）

再如，**主成分估计**——可以去掉一些复共线性
# 小结
采用回归模型进行建模的可取步骤如下：
1. 建立回归模型
	确立样本空间，对数据进行标准化处理，采用逐步回归法筛选自变量
